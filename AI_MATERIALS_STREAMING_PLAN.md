# AI Materials Page Real-time Streaming Implementation Plan

## Overview
The goal is to implement real-time streaming for the AI Materials page so that users can see suggestions as they are generated by the AI, rather than waiting for the complete response.

## Current State Analysis
- The frontend at `app/ai-materials/page.tsx` makes a regular POST request to `/ai/materials`
- The backend route in `Backend/routes.py` (lines 948-956) makes a regular API call
- The AI service in `Backend/ai_design_service.py` has a method that's supposed to stream but has bugs:
  - Missing `json` import causing errors on line 179
  - The method returns text chunks but the route doesn't handle streaming
  - The frontend doesn't consume streaming responses

## Implementation Plan

### Phase 1: Backend Implementation

#### 1.1 Fix AI Design Service
- Add missing `import json` to `Backend/ai_design_service.py`
- Modify `get_material_suggestions` method to properly handle streaming
- Instead of returning text chunks, implement a proper streaming mechanism that sends complete JSON objects as they become available
- Handle the Groq streaming response and build up the JSON response incrementally

#### 1.2 Update Backend Route
- Modify the `/ai/materials` route in `Backend/routes.py` to use `StreamingResponse`
- The route should accept the MaterialRequest and return a StreamingResponse that consumes the async generator from the AI service
- Ensure proper content-type is set for streaming JSON

### Phase 2: Frontend Implementation

#### 2.1 Update Frontend API Call
- Modify the `getMaterialSuggestions` function in `app/ai-materials/page.tsx` to consume the streaming response
- Use `fetch` with response streaming to read data as it comes in
- Implement a `ReadableStream` reader to process the incoming data

#### 2.2 Update UI for Real-time Display
- Modify the UI to display partial results as they arrive
- Add loading indicators that show the streaming progress
- Update the suggestions display area to incrementally show results
- Handle the streaming response format properly (likely Server-Sent Events format)

### Phase 3: Testing and Optimization

#### 3.1 Test Streaming Functionality
- Verify that partial results are displayed in real-time
- Test with various room types and styles
- Ensure error handling works properly during streaming

#### 3.2 Performance Optimization
- Optimize the streaming response format for best user experience
- Consider adding progress indicators
- Ensure the UI updates smoothly without flickering

## Technical Details

### Backend Changes

The streaming implementation should follow this pattern:

```python
@app.post("/ai/materials-stream")
async def stream_material_suggestions(request: MaterialRequest):
    """Stream AI-powered material suggestions in real-time"""
    return StreamingResponse(
        ai_design_service.stream_material_suggestions(request),
        media_type="text/event-stream"
    )
```

The AI service method should be modified to:

1. Properly handle the Groq streaming response
2. Accumulate the response until a complete JSON object is formed
3. Send the complete JSON object as a data chunk in the stream
4. Handle errors gracefully during streaming

### Frontend Changes

The frontend should use a pattern like:

```javascript
const response = await fetch('/api/ai/materials-stream', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify(materialRequest)
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  // Process the chunk and update UI
}
```

## Expected Benefits
- Users see results as they are generated rather than waiting for completion
- Better user experience with real-time feedback
- More responsive interface during AI processing
- Improved perceived performance

## Success Criteria
- Streaming response delivers partial results in real-time
- Frontend updates UI incrementally as data arrives
- Error handling works during streaming
- Performance is optimized for smooth user experience
- All existing functionality remains intact